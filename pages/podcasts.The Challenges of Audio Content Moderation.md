- retrieved:: [[2022/04/07]]
  author:: [[The Lawfare Podcast]]
  category:: [[podcasts]]
  source:: [[airr]]
  tags:: 
  referrer::readwise
- ![book_image](https://ssl-static.libsyn.com/p/assets/2/0/0/2/2002e09fb68936e9/Trial_1.png){:height 200 :width 200}
- ## [[Readwise Highlights]]
	- link:: [https://www.airr.io/quote/60816bb623577f61aa3a7e95](https://www.airr.io/quote/60816bb623577f61aa3a7e95)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: rooms and it's really hard because for me, I'm like well I can't verify that, right? And I don't know if clubhouse can now verify that, given that the room presumably is over, it sort of happened and it may have injured the people inside that room, but all anyone else has our our second hand reports of it. So it's this it's I think it's the sword that cuts both ways. Right on one hand, the blast radius is smaller. You can't like, you know, retweet this terrible thing and have it go to millions or billions of people on the other hand, however right, it means that the people that are in that room are potentially the only people that are able to report this thing and if you are doing a bad thing, maybe you can evade enforcement just by effectively keeping your room short and doing it over and over. 
	  
	  Speaker 1: Yes. Although I
	- link:: [https://www.airr.io/quote/60816bd141de78f374cd79d0](https://www.airr.io/quote/60816bd141de78f374cd79d0)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: room short and doing it over and over. 
	  
	  Speaker 1: Yes. Although I do think that there is a really important conversation to be had that we've barely started yet about do we really want the same standards for content moderation or trust and safety across every platform and every kind of afford inst like it might be that we have different standards for text that's forever and sitting there and ephemeral content which is you know, as you said, talking to each other online and so it's not clear to me that you know, we necessarily want uniformity across all of the different platform types, but we've barely started that conversation yet. So I expect that to be something that we explore more in coming years. One of the things
	- link:: [https://www.airr.io/quote/60816bea41de7804afcd79d2](https://www.airr.io/quote/60816bea41de7804afcd79d2)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 1: that we explore more in coming years. One of the things that you were talking about just then about being able to verify if something was said and also the scalability of content moderation in audio. One of things that that gets to is because it's not necessarily realistic or desirable to have platform moderators sitting and listening in on every conversation that happens on the platform to make sure that nothing that infringes the guidelines is said. And because automated tools as we were exploring, is so much blunter and still basically inadequate and not up to the task, a lot more of the responsibility falls to the channel and server moderators and the community in general. And
	- link:: [https://www.airr.io/quote/60816d1e23577f308e3a7ec6](https://www.airr.io/quote/60816d1e23577f308e3a7ec6)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: call back to, for example, the the paradox of tolerance here, Right. And and those ideas, but I think that the fundamental sort of nature of it is that if you sort of push this and there was some discussion actually in Inside Channel yesterday about this, if you push this to sort of the middleware and you're like, hey, I'm going to let anything go. And then there's, there's this sort of middle layer, right of software or what have you, where you can choose to customize your experience like Yes, in one sense you're, you're pushing that responsibility onto your users and you're allowing users perhaps with good tooling to customize exactly how their experiences want to be. I think the real question though is like, right shouldn't you as the government be setting a floor to what that acceptable behavior is, right? Like at some level, I think the
	- link:: [https://www.airr.io/quote/60816d2a41de785224cd7a05](https://www.airr.io/quote/60816d2a41de785224cd7a05)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: right of software or what have you, where you can choose to customize your experience like Yes, in one sense you're, you're pushing that responsibility onto your users and you're allowing users perhaps with good tooling to customize exactly how their experiences want to be. I think the real question though is like, right shouldn't you as the government be setting a floor to what that acceptable behavior is, right? Like at some level, I think the real question is is it reasonable for any platform to say ah yes. Like you can say you can make death threats because if the user wants to opt out of it, they can just opt out of it. Like that feels wrong to me personally, because I think that the platform, the government has a responsibility to step in. I don't think that that
	- link:: [https://www.airr.io/quote/60816d4b23577f86583a7ec8](https://www.airr.io/quote/60816d4b23577f86583a7ec8)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: that right, the ability and and maybe the sort of optimal nature for me is a government that protects users in a way where they feel comfortable, they feel safe in expressing themselves. And that is I think just not status quo and a lot of these major platforms right now that if you are a minority, for example, you do not feel safe necessarily expressing yourself because when you do you immediately get all sorts of like vicious harassment. Would the world be better if those people had tools to better sort of save themselves? Like? I think so. Yes. Right. And there are initiatives like block party is one of the apps, I think that is currently in stealth, that sort of aims to do some of that, but I think the fundamental responsibility should actually lie on the platform itself.
	- link:: [https://www.airr.io/quote/60816d8823577f85e43a7eca](https://www.airr.io/quote/60816d8823577f85e43a7eca)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 2: wondering, you know, isn't that basically just the audio equivalent of the twitter mute function? But I wanted to ask you, you know, what do you think of tools like these and what is the capability of automated moderation when it comes to audio? 
	  
	  Speaker 0: There's definitely a lot of research going into it. I'm reasonably familiar with the with the intel offering. I mean, I think that part of it and this is just sort of generally from what I know in most cases what's happening is that they're taking the audio and they're doing a transcribe right into text and then they're running the ai on the text. And so it sort of goes back to that earlier conversation. You lose like some of the tone that exists, right? You lose some of the emphasis that that exists, right? It's it's a very different
	- link:: [https://www.airr.io/quote/60816d9541de78513acd7a0c](https://www.airr.io/quote/60816d9541de78513acd7a0c)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: I think that part of it and this is just sort of generally from what I know in most cases what's happening is that they're taking the audio and they're doing a transcribe right into text and then they're running the ai on the text. And so it sort of goes back to that earlier conversation. You lose like some of the tone that exists, right? You lose some of the emphasis that that exists, right? It's it's a very different statement if I say, I'm gonna kill someone today and you know, I'm going to kill someone today, right? And I think that's something that most to my knowledge, maybe none of the current sort of Ai systems that exist do that. They're all sort of taking that, turning it into texas and saying, okay, is the text violative? So I don't think the ai is quite there yet, but I think it's sort of like even if we're looking at just like
	- link:: [https://www.airr.io/quote/60816db241de7899cacd7a13](https://www.airr.io/quote/60816db241de7899cacd7a13)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: ai right? I think that context is still incredibly hard for for the Ai systems that everyone you know has the ability to sort of purchase or that everyone is sort of implementing right now. I think it's still hard because you can study all of the data in the world and and see these like billions of interactions. But it turns out that like human interaction, right? Is complex and is frequently subtle. And if someone is using a specific word right, like they may be using that word as a racial slur, they may be using that word as part of a rap lyric, they may be using that word as a member of the in group. They may be using that word in sort of a journalistic sense. Right? And I think it's that context that is really hard. And
	- link:: [https://www.airr.io/quote/60816dd023577f6b243a7ed2](https://www.airr.io/quote/60816dd023577f6b243a7ed2)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: you know, facebook's transparency reports. I think you can see this sort of from all of the vendors out there, like the ai that exists is going to be overbroad, it is going to take down things that it should not take down. And I think it is just a sort of a question right of whether, I think most companies are okay with that because it turns out that there's no, you know, there's no there's no liability for taking things down when you didn't need to take them down. And I think in a lot of cases, in the trust and safety space, you would rather have taken something down, right that you needed to take down rather than leave something up. And so it's a little bit disappointing. I think in some cases that that is sort of the default model, but that does feel like the default and current model and I'm not sure that that's going to change anytime soon.
	- link:: [https://www.airr.io/quote/60816ddc23577f1e553a7ed4](https://www.airr.io/quote/60816ddc23577f1e553a7ed4)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: because it turns out that there's no, you know, there's no there's no liability for taking things down when you didn't need to take them down. And I think in a lot of cases, in the trust and safety space, you would rather have taken something down, right that you needed to take down rather than leave something up. And so it's a little bit disappointing. I think in some cases that that is sort of the default model, but that does feel like the default and current model and I'm not sure that that's going to change anytime soon. 
	  
	  Speaker 1: So discord saw a big jump in violent extremism takedowns in the second half of 2020, the team proactively removed 1,504 servers for violent extremism in the second half of 2020, which was a nearly 93 increase from the first half of the year.
	- link:: [https://www.airr.io/quote/60816de223577f25143a7ed7](https://www.airr.io/quote/60816de223577f25143a7ed7)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: have taken something down, right that you needed to take down rather than leave something up. And so it's a little bit disappointing. I think in some cases that that is sort of the default model, but that does feel like the default and current model and I'm not sure that that's going to change anytime soon. 
	  
	  Speaker 1: So discord saw a big jump in violent extremism takedowns in the second half of 2020, the team proactively removed 1,504 servers for violent extremism in the second half of 2020, which was a nearly 93 increase from the first half of the year. The transparency report noted that this increase can be attributed to the expansion of our anti extremism efforts as well as growing trends in the online extremism space. And I'm wondering
	- link:: [https://www.airr.io/quote/6081702e41de7833afcd7a3e](https://www.airr.io/quote/6081702e41de7833afcd7a3e)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: and it is always hard I think in situations like this because when you say, oh yes, like we used to you know remove this amount and now we remove this amount. There's always this question of like, okay, does that mean that the activity itself drastically increased or did you just get better at catching it and you weren't catching it before? And you know, I don't work there anymore and I don't have a very good guess as to what that is, but I I think, you know, in almost all cases it is going to be a little bit of Coleman and a little bit of Colombia, right, you put more resources into finding it, maybe you're able to sort of like investigate more deeply in certain cases, right? At the same time as the world is reacting and maybe there is more of it than there was to find. 
	  
	  Speaker 2: So given that you're both a
	- link:: [https://www.airr.io/quote/6081708723577f0d613a7eeb](https://www.airr.io/quote/6081708723577f0d613a7eeb)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: the big players have much more of it to deal with, but again, they can throw a billion dollars at it and then effectively consider it dealt with, Right? I think that for these small, for the small startups out there for the for the non incumbents, right? For the companies that are, you know, maybe in this sort of medium space where you have some users, but you're still trying to, you know, figure things out, you're still trying to figure out like what your company will look like over the next couple of years. I think it is a much, it is just much harder. And I think that certainly I've seen hopefully not very successful bills that would effectively, you know, legislate Companies like to sort out of existence. So I do think that that is a very real danger. I think there are, you know, there's a lot of ink that's been shed
	- link:: [https://www.airr.io/quote/608163ec23577faf2f3a7e38](https://www.airr.io/quote/608163ec23577faf2f3a7e38)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: it's kind of, I guess for us at least it was very much sort of the idea of like a unified single platform that allows for this like easy drop in, drop out. And then also for this continuous background of communication, 
	  
	  Speaker 2: you are the head of trust and safety at 
	  
	  Speaker 0: discord until very recently, and I 
	  
	  Speaker 2: think trust and safety is sort of an industry euphemism right now, which may soon become as widely known as, you know, CFO or something like that, but for right now it's probably a term that many people haven't heard. Can you just tell us what it means in normal english 
	  
	  Speaker 0: in in normal english trust and safety is effectively being responsible for all of the bad things that happen on the platform.
	- link:: [https://www.airr.io/quote/6081640441de789012cd7987](https://www.airr.io/quote/6081640441de789012cd7987)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 2: normal english 
	  
	  Speaker 0: in in normal english trust and safety is effectively being responsible for all of the bad things that happen on the platform. Uh it ends up being sort of, I think the majority of it ends up being handling effectively user disputes right situations where when you have a user generated content platform, when you have a platform where people are interacting with each other, of course, some of those people are not going to get along, some of those people are not going to be very nice people. And so trust and safety is the function that sort of handles that, that resolves those disputes, that make sure that users, the people are not hurting each other, that also they are not hurting the platform itself. I think in
	- link:: [https://www.airr.io/quote/6081643841de78be13cd798e](https://www.airr.io/quote/6081643841de78be13cd798e)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 1: over the course of your time there, 
	  
	  Speaker 0: I think pretty much all companies that deal with this sort of content at least go through a kind of a similar evolutionary cycle where you start out with a support organization, you have a customer support organization that handles sort of password requests, account request, sort of general stuff like that. And I think the more sort of UGC you have, the more support starts to have to handle those issues as your platform gets bigger as you hit. I don't know, 100,000 users, Maybe a million users. You start seeing some of those issues where people will write in and they will be like, hey, this person said that's like super, you know, offensive thing or this person is doing hate speech or whatever the case
	- link:: [https://www.airr.io/quote/6081643e23577f866a3a7e3b](https://www.airr.io/quote/6081643e23577f866a3a7e3b)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 1: over the course of your time there, 
	  
	  Speaker 0: I think pretty much all companies that deal with this sort of content at least go through a kind of a similar evolutionary cycle where you start out with a support organization, you have a customer support organization that handles sort of password requests, account request, sort of general stuff like that. And I think the more sort of UGC you have, the more support starts to have to handle those issues as your platform gets bigger as you hit. I don't know, 100,000 users, Maybe a million users. You start seeing some of those issues where people will write in and they will be like, hey, this person said that's like super, you know, offensive thing or this person is doing hate speech or whatever the case
	- link:: [https://www.airr.io/quote/6081644941de78268fcd7992](https://www.airr.io/quote/6081644941de78268fcd7992)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: pretty much all companies that deal with this sort of content at least go through a kind of a similar evolutionary cycle where you start out with a support organization, you have a customer support organization that handles sort of password requests, account request, sort of general stuff like that. And I think the more sort of UGC you have, the more support starts to have to handle those issues as your platform gets bigger as you hit. I don't know, 100,000 users, Maybe a million users. You start seeing some of those issues where people will write in and they will be like, hey, this person said that's like super, you know, offensive thing or this person is doing hate speech or whatever the case may be. And it's usually, I think about that point in time when
	- link:: [https://www.airr.io/quote/6081647f41de7876c4cd7994](https://www.airr.io/quote/6081647f41de7876c4cd7994)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: there's a press cycle or a news article or something that says, hey, this company is not sort of thinking about it. So I think that's that's the general genesis and it certainly was for discord in terms of changes. Who, I mean, I think the most interesting thing, right, is that especially over the past half a decade, the societal expectations, and I think the general actually like understandings of trust and safety have grown. And I think as a result of that much more, what I would sort of label libertarian ideals of maybe a decade ago when it was fine to say, you know what, we're just a speech platform. All speeches, Good speech Right, That's the end of it has definitely shifted. Right.
	- link:: [https://www.airr.io/quote/608164e423577f736f3a7e3d](https://www.airr.io/quote/608164e423577f736f3a7e3d)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: trust and safety has had to come up with more policies, more processes, more enforcement to understand what should be taken down and how to effectively go about it. 
	  
	  Speaker 1: So, our podcast is titled Arbiters of Truth in a
	- link:: [https://www.airr.io/quote/6081656423577fea863a7e40](https://www.airr.io/quote/6081656423577fea863a7e40)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: of of your your platform are are happy, are are healthy, are safe. I think safety of course always comes first. I I do think that there is this really interesting, like I think in some sense, right, like sort of looking at as that that sort of government analogy. I think one of the most interesting things is that you can often operate on your policies faster than a normal government would be able to. So you can see that a new harm is coming around the horizon. Maybe it's deepfakes, maybe it's some specific type of misinformation, maybe it's a specific bad actor, right? Um, some nation state actors doing in certain thing here. And you can iterate on sort of how to respond to that extremely quickly
	- link:: [https://www.airr.io/quote/6081676c23577fcb0b3a7e62](https://www.airr.io/quote/6081676c23577fcb0b3a7e62)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: to come in make intelligent, good policies that is based on real world, scientific, sociological, historical data and then to try and make the best society that they think that can exist, right? The most inclusive. The one where the most people feel like they belong. 
	  
	  Speaker 2: One of the interesting things about this space is that people who maybe haven't thought so much about it often will respond to descriptions of difficult cases by saying sort of, well, you know, it's it's so easy, right? Like just just take the bad stuff down and fix it. Um Are there particular cases that stand out in your mind is challenging? Uh huh. 
	  
	  Speaker 0: Just, you know, every every day, every minute, maybe. No, I I see that all
	- link:: [https://www.airr.io/quote/6081684441de782716cd79b4](https://www.airr.io/quote/6081684441de782716cd79b4)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 1: and throw rocks, I would not want to be in that chair in any circumstance, I don't think. And to make it worse. Trust and safety issues are also famously haunting. Like, your job is often to stare at the worst of the internet and of humanity all day every day and to try and make sure that the rest of us don't have to see it. What was that like 
	  
	  Speaker 3: and how 
	  
	  Speaker 1: I don't want to ask you, Like, how did you find the will to live while you were working there? But like how do you find fulfillment? What what makes it worth it to go through something so horrific? 
	  
	  Speaker 0: I mean, I think so, so a couple of thoughts, There are one of the things is that I don't know that it has to be sort of all horrific all the time. I
	- link:: [https://www.airr.io/quote/6081688941de786160cd79b6](https://www.airr.io/quote/6081688941de786160cd79b6)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: to not great outcomes right? Casey noon I think has written sort of a great length about this and I think very rightfully so. And I think that there are sort of like conditions that are very hard. I don't know that all conditions in trust and safety are like that. So I think that's the first thing that I would say. The second thing though, is that again sort of I guess returning to to the government metaphor, I guess it's not really a metaphor. I guess the way that I put it is that you know, I think that in the same way that some of our, you know, our law enforcement officials, members of the judiciary right all across the executive branch, like, they see some pretty terrible things as well and, or are charged with right, like their job is to prosecute those terrible things. And that requires pretty up close and
	- link:: [https://www.airr.io/quote/608168cc23577fdae43a7e6e](https://www.airr.io/quote/608168cc23577fdae43a7e6e)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: has. And I think that in large part that is because of that, you derive some, some fulfillment. I think some enjoyment, some uh, satisfaction, I guess, is how I would put it over doing the right thing, right? That at the end of the day, the trust and safety job is not, or at least doesn't have to be just a like, oh, yes, I am, you know, in the worst case scenario, again, like looking at porn or beheading videos or whatever all day, but instead it really is and I think the goal should be right to create in industry and a culture and a job function where it's like, what are we doing to protect people? Right? That technology is this tool that has allowed for great
	- link:: [https://www.airr.io/quote/6081690423577f5de33a7e70](https://www.airr.io/quote/6081690423577f5de33a7e70)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: has allowed people to find each other ideas to flourish, technology developed etcetera etcetera. Right at the same time though, as with many tools there is a serious, I think potential is not even the right word here, there's just serious misuse out there and trust and safety sort of exist to to protect people. And so I think I, you know, I wake up in the mornings and I am happy to do the job because I think that it is about trying to make the society a better place, trying to put in place the policies, the procedures, the support the tools, the technology, though, whatever it is right to make sure that these sort of like bad behaviors, right? These behaviors that hurt other people, they go down.
	- link:: [https://www.airr.io/quote/6081698241de7889ddcd79c4](https://www.airr.io/quote/6081698241de7889ddcd79c4)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: the tools, the technology, though, whatever it is right to make sure that these sort of like bad behaviors, right? These behaviors that hurt other people, they go down. And I think every time that someone writes in and says, Hey, I've been hurt and you can respond to that person and say, you know, we took X action, we did why? Whatever. And that person says, Hey, thank you so much. Like this was, this was really weighing on me and, and I'm glad to see that you're making this better and I feel happy participating, right? Like I think that is, that's what we all go for, right? We want to minimize the bad, We want to maximize the good. And I think this is sort of a fundamental and necessary part of that, that, that in the same way that government is a fundamental and necessary part of humanity and society. 
	  
	  Speaker 1: So people don't only
	- link:: [https://www.airr.io/quote/60816a4123577f6a763a7e7a](https://www.airr.io/quote/60816a4123577f6a763a7e7a)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: about what you're building the product for and what your feedback loops are around the product. And so I think one of the things that I would say is that if you are building like add supported products than to your to your comment, right? Like you are generally building for more engagement, more engagement means more engagement across the board. And, and again, human beings love their polarization and they're polarized engagement. And so if you know you as a product manager, you're building something and you're looking at, you're like, hey, like what I'm optimizing here for is eyeballs and time on site. I think inherently unless you build sort of stops to prevent that you are going to be driving more sort of extreme rhetoric and content. I think if you don't
	- link:: [https://www.airr.io/quote/60816b0741de780e4dcd79cb](https://www.airr.io/quote/60816b0741de780e4dcd79cb)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: Right? I think just at the top I think let me break down the sort of basics and then we can talk a little bit about. I think the challenges the basics are that audio carries a lot more information than text us. Right. Text, you you get all of it, you have it easily encoded, it is like relatively short, it is relatively compact. It is very package Izabal, you can store lots of text very cheaply. Right? And so everyone does. It's sort of a it's a very known quantity in a lot of plays audio if nothing else includes tone as a piece of information. Right. It's encoding requires much more data and it is going to be much more unique. And so right, from just a, like how are you storing the data perspective? It is like a difference on a order of magnitude plus on the are you
	- link:: [https://www.airr.io/quote/60816b3523577f9b813a7e88](https://www.airr.io/quote/60816b3523577f9b813a7e88)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: bad words right? Where you're like, hey, I don't I don't want this racial slur to appear. I can block that. That's like very easy, very sort of low level technology all the way up into your ai Ml driven systems where they've analyzed, you know, a couple of billion words and they say, okay, this statement is bad and those are reasonable at the moment. That body doesn't really exist on the voice side. And so there's no real voice platform. Certainly no third party is sort of like voice moderation platform that says yes, I can come in analyze all of your voice real time and tell you who's doing, who's who's engaging in hate speech or who is engaging in a threat for example. So I think that's one of the sort of big difficulties around it. I think the other thing is that right? Like it
	- link:: [https://www.airr.io/quote/60816b4b23577fcfb83a7e8a](https://www.airr.io/quote/60816b4b23577fcfb83a7e8a)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: who's doing, who's who's engaging in hate speech or who is engaging in a threat for example. So I think that's one of the sort of big difficulties around it. I think the other thing is that right? Like it is really just the difference between like digital and a lot of context, right? If you are a trust and safety, this is operationally focused. If you're a trust and safety employee, you can scan through a conversation that maybe spans, I don't know, a couple hours, you can do an investigation on someone's account Pretty quickly. If you're, you know, in a clubhouse session and someone's like, hey, someone said a bad thing, I don't know somewhere around the 25-30 minute mark. You're listening to that for at least you know, the five minutes, right? If not more than that, you probably want to listen to a little bit more than that for context. That is sort of a limiting factor. So
	- link:: [https://www.airr.io/quote/60816b5f23577f36663a7e8c](https://www.airr.io/quote/60816b5f23577f36663a7e8c)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: don't know somewhere around the 25-30 minute mark. You're listening to that for at least you know, the five minutes, right? If not more than that, you probably want to listen to a little bit more than that for context. That is sort of a limiting factor. So I, I think fundamentally part of it is that clubhouse really was the Like audio has been around in a lot of these spaces for a long time to, to to your points, right? Like it's not like we were suddenly the guy is like, I have something brand new. It's called talking to each other. But I think historically it has mostly been either 1-1 or it has been in A. B, two B a business to business context. Right? And so in both of those cases, sort of your content moderation issues are lower. I think now with, with
	- link:: [https://www.airr.io/quote/60816b7723577f07433a7e8f](https://www.airr.io/quote/60816b7723577f07433a7e8f)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: of those cases, sort of your content moderation issues are lower. I think now with, with clubhouse, especially now everyone's like, hey, there's a lot of interest in that particular type of engagement where you have a, you know, some people are the host, but anyone can join, it could potentially go to anyone and it's open to anyone. I think we are seeing this sort of like new wave of content moderation issues and I strongly suspect right, like every company will have to deal with these on their own spaces. I mean, I for one am very curious how they will respond to this because you can imagine, right, like Text is scalable to 10,000 rows or 10 million messages in a way that audio is not scalable if you have 10,000 rooms going on at the same time,
	- link:: [https://www.airr.io/quote/60816ba623577f325c3a7e92](https://www.airr.io/quote/60816ba623577f325c3a7e92)
	  on:: [[2021/04/22]]
	  tags:: 
	  Speaker 0: this or someone should have done something about this, right? Like that text last forever with these audio spaces that are sort of right? Like these, I guess they are conferences effectively right like that are happening sort of ephemeral e hypothetically you're blast radius is actually much lesson, it's only the people in the cause that are that are hearing it. I mean very I actually saw it on twitter this morning, there's been some reports of some more anti Semitic clubhouse rooms and it's really hard because for me, I'm like well I can't verify that, right? And I don't know if clubhouse can now verify that, given that the room presumably is over, it sort of happened and it may have injured the people inside that room, but all anyone else has our our second hand reports of it. So it's
	- link:: [https://www.airr.io/quote/608163ec23577faf2f3a7e38](https://www.airr.io/quote/608163ec23577faf2f3a7e38)
	  on:: [[2021/04/23]]
	  tags:: [[discard]]
	  Speaker 0: it's kind of, I guess for us at least it was very much sort of the idea of like a unified single platform that allows for this like easy drop in, drop out. And then also for this continuous background of communication, 
	  
	  Speaker 2: you are the head of trust and safety at 
	  
	  Speaker 0: discord until very recently, and I 
	  
	  Speaker 2: think trust and safety is sort of an industry euphemism right now, which may soon become as widely known as, you know, CFO or something like that, but for right now it's probably a term that many people haven't heard. Can you just tell us what it means in normal english 
	  
	  Speaker 0: in in normal english trust and safety is effectively being responsible for all of the bad things that happen on the platform.
	- link:: [https://www.airr.io/quote/6081640441de789012cd7987](https://www.airr.io/quote/6081640441de789012cd7987)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 2: normal english 
	  
	  Speaker 0: in in normal english trust and safety is effectively being responsible for all of the bad things that happen on the platform. Uh it ends up being sort of, I think the majority of it ends up being handling effectively user disputes right situations where when you have a user generated content platform, when you have a platform where people are interacting with each other, of course, some of those people are not going to get along, some of those people are not going to be very nice people. And so trust and safety is the function that sort of handles that, that resolves those disputes, that make sure that users, the people are not hurting each other, that also they are not hurting the platform itself. I think in
	- link:: [https://www.airr.io/quote/6081643841de78be13cd798e](https://www.airr.io/quote/6081643841de78be13cd798e)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 1: over the course of your time there, 
	  
	  Speaker 0: I think pretty much all companies that deal with this sort of content at least go through a kind of a similar evolutionary cycle where you start out with a support organization, you have a customer support organization that handles sort of password requests, account request, sort of general stuff like that. And I think the more sort of UGC you have, the more support starts to have to handle those issues as your platform gets bigger as you hit. I don't know, 100,000 users, Maybe a million users. You start seeing some of those issues where people will write in and they will be like, hey, this person said that's like super, you know, offensive thing or this person is doing hate speech or whatever the case
	- link:: [https://www.airr.io/quote/6081643e23577f866a3a7e3b](https://www.airr.io/quote/6081643e23577f866a3a7e3b)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 1: over the course of your time there, 
	  
	  Speaker 0: I think pretty much all companies that deal with this sort of content at least go through a kind of a similar evolutionary cycle where you start out with a support organization, you have a customer support organization that handles sort of password requests, account request, sort of general stuff like that. And I think the more sort of UGC you have, the more support starts to have to handle those issues as your platform gets bigger as you hit. I don't know, 100,000 users, Maybe a million users. You start seeing some of those issues where people will write in and they will be like, hey, this person said that's like super, you know, offensive thing or this person is doing hate speech or whatever the case
	- link:: [https://www.airr.io/quote/6081644941de78268fcd7992](https://www.airr.io/quote/6081644941de78268fcd7992)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: pretty much all companies that deal with this sort of content at least go through a kind of a similar evolutionary cycle where you start out with a support organization, you have a customer support organization that handles sort of password requests, account request, sort of general stuff like that. And I think the more sort of UGC you have, the more support starts to have to handle those issues as your platform gets bigger as you hit. I don't know, 100,000 users, Maybe a million users. You start seeing some of those issues where people will write in and they will be like, hey, this person said that's like super, you know, offensive thing or this person is doing hate speech or whatever the case may be. And it's usually, I think about that point in time when
	- link:: [https://www.airr.io/quote/6081647f41de7876c4cd7994](https://www.airr.io/quote/6081647f41de7876c4cd7994)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: there's a press cycle or a news article or something that says, hey, this company is not sort of thinking about it. So I think that's that's the general genesis and it certainly was for discord in terms of changes. Who, I mean, I think the most interesting thing, right, is that especially over the past half a decade, the societal expectations, and I think the general actually like understandings of trust and safety have grown. And I think as a result of that much more, what I would sort of label libertarian ideals of maybe a decade ago when it was fine to say, you know what, we're just a speech platform. All speeches, Good speech Right, That's the end of it has definitely shifted. Right.
	- link:: [https://www.airr.io/quote/608164e423577f736f3a7e3d](https://www.airr.io/quote/608164e423577f736f3a7e3d)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: trust and safety has had to come up with more policies, more processes, more enforcement to understand what should be taken down and how to effectively go about it. 
	  
	  Speaker 1: So, our podcast is titled Arbiters of Truth in a
	- link:: [https://www.airr.io/quote/6081656423577fea863a7e40](https://www.airr.io/quote/6081656423577fea863a7e40)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: of of your your platform are are happy, are are healthy, are safe. I think safety of course always comes first. I I do think that there is this really interesting, like I think in some sense, right, like sort of looking at as that that sort of government analogy. I think one of the most interesting things is that you can often operate on your policies faster than a normal government would be able to. So you can see that a new harm is coming around the horizon. Maybe it's deepfakes, maybe it's some specific type of misinformation, maybe it's a specific bad actor, right? Um, some nation state actors doing in certain thing here. And you can iterate on sort of how to respond to that extremely quickly
	- link:: [https://www.airr.io/quote/6081676c23577fcb0b3a7e62](https://www.airr.io/quote/6081676c23577fcb0b3a7e62)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: to come in make intelligent, good policies that is based on real world, scientific, sociological, historical data and then to try and make the best society that they think that can exist, right? The most inclusive. The one where the most people feel like they belong. 
	  
	  Speaker 2: One of the interesting things about this space is that people who maybe haven't thought so much about it often will respond to descriptions of difficult cases by saying sort of, well, you know, it's it's so easy, right? Like just just take the bad stuff down and fix it. Um Are there particular cases that stand out in your mind is challenging? Uh huh. 
	  
	  Speaker 0: Just, you know, every every day, every minute, maybe. No, I I see that all
	- link:: [https://www.airr.io/quote/6081684441de782716cd79b4](https://www.airr.io/quote/6081684441de782716cd79b4)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 1: and throw rocks, I would not want to be in that chair in any circumstance, I don't think. And to make it worse. Trust and safety issues are also famously haunting. Like, your job is often to stare at the worst of the internet and of humanity all day every day and to try and make sure that the rest of us don't have to see it. What was that like 
	  
	  Speaker 3: and how 
	  
	  Speaker 1: I don't want to ask you, Like, how did you find the will to live while you were working there? But like how do you find fulfillment? What what makes it worth it to go through something so horrific? 
	  
	  Speaker 0: I mean, I think so, so a couple of thoughts, There are one of the things is that I don't know that it has to be sort of all horrific all the time. I
	- link:: [https://www.airr.io/quote/6081688941de786160cd79b6](https://www.airr.io/quote/6081688941de786160cd79b6)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: to not great outcomes right? Casey noon I think has written sort of a great length about this and I think very rightfully so. And I think that there are sort of like conditions that are very hard. I don't know that all conditions in trust and safety are like that. So I think that's the first thing that I would say. The second thing though, is that again sort of I guess returning to to the government metaphor, I guess it's not really a metaphor. I guess the way that I put it is that you know, I think that in the same way that some of our, you know, our law enforcement officials, members of the judiciary right all across the executive branch, like, they see some pretty terrible things as well and, or are charged with right, like their job is to prosecute those terrible things. And that requires pretty up close and
	- link:: [https://www.airr.io/quote/608168cc23577fdae43a7e6e](https://www.airr.io/quote/608168cc23577fdae43a7e6e)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: has. And I think that in large part that is because of that, you derive some, some fulfillment. I think some enjoyment, some uh, satisfaction, I guess, is how I would put it over doing the right thing, right? That at the end of the day, the trust and safety job is not, or at least doesn't have to be just a like, oh, yes, I am, you know, in the worst case scenario, again, like looking at porn or beheading videos or whatever all day, but instead it really is and I think the goal should be right to create in industry and a culture and a job function where it's like, what are we doing to protect people? Right? That technology is this tool that has allowed for great
	- link:: [https://www.airr.io/quote/6081690423577f5de33a7e70](https://www.airr.io/quote/6081690423577f5de33a7e70)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: has allowed people to find each other ideas to flourish, technology developed etcetera etcetera. Right at the same time though, as with many tools there is a serious, I think potential is not even the right word here, there's just serious misuse out there and trust and safety sort of exist to to protect people. And so I think I, you know, I wake up in the mornings and I am happy to do the job because I think that it is about trying to make the society a better place, trying to put in place the policies, the procedures, the support the tools, the technology, though, whatever it is right to make sure that these sort of like bad behaviors, right? These behaviors that hurt other people, they go down.
	- link:: [https://www.airr.io/quote/6081698241de7889ddcd79c4](https://www.airr.io/quote/6081698241de7889ddcd79c4)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: the tools, the technology, though, whatever it is right to make sure that these sort of like bad behaviors, right? These behaviors that hurt other people, they go down. And I think every time that someone writes in and says, Hey, I've been hurt and you can respond to that person and say, you know, we took X action, we did why? Whatever. And that person says, Hey, thank you so much. Like this was, this was really weighing on me and, and I'm glad to see that you're making this better and I feel happy participating, right? Like I think that is, that's what we all go for, right? We want to minimize the bad, We want to maximize the good. And I think this is sort of a fundamental and necessary part of that, that, that in the same way that government is a fundamental and necessary part of humanity and society. 
	  
	  Speaker 1: So people don't only
	- link:: [https://www.airr.io/quote/60816a4123577f6a763a7e7a](https://www.airr.io/quote/60816a4123577f6a763a7e7a)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: about what you're building the product for and what your feedback loops are around the product. And so I think one of the things that I would say is that if you are building like add supported products than to your to your comment, right? Like you are generally building for more engagement, more engagement means more engagement across the board. And, and again, human beings love their polarization and they're polarized engagement. And so if you know you as a product manager, you're building something and you're looking at, you're like, hey, like what I'm optimizing here for is eyeballs and time on site. I think inherently unless you build sort of stops to prevent that you are going to be driving more sort of extreme rhetoric and content. I think if you don't
	- link:: [https://www.airr.io/quote/60816b0741de780e4dcd79cb](https://www.airr.io/quote/60816b0741de780e4dcd79cb)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: Right? I think just at the top I think let me break down the sort of basics and then we can talk a little bit about. I think the challenges the basics are that audio carries a lot more information than text us. Right. Text, you you get all of it, you have it easily encoded, it is like relatively short, it is relatively compact. It is very package Izabal, you can store lots of text very cheaply. Right? And so everyone does. It's sort of a it's a very known quantity in a lot of plays audio if nothing else includes tone as a piece of information. Right. It's encoding requires much more data and it is going to be much more unique. And so right, from just a, like how are you storing the data perspective? It is like a difference on a order of magnitude plus on the are you
	- link:: [https://www.airr.io/quote/60816b3523577f9b813a7e88](https://www.airr.io/quote/60816b3523577f9b813a7e88)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: bad words right? Where you're like, hey, I don't I don't want this racial slur to appear. I can block that. That's like very easy, very sort of low level technology all the way up into your ai Ml driven systems where they've analyzed, you know, a couple of billion words and they say, okay, this statement is bad and those are reasonable at the moment. That body doesn't really exist on the voice side. And so there's no real voice platform. Certainly no third party is sort of like voice moderation platform that says yes, I can come in analyze all of your voice real time and tell you who's doing, who's who's engaging in hate speech or who is engaging in a threat for example. So I think that's one of the sort of big difficulties around it. I think the other thing is that right? Like it
	- link:: [https://www.airr.io/quote/60816b4b23577fcfb83a7e8a](https://www.airr.io/quote/60816b4b23577fcfb83a7e8a)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: who's doing, who's who's engaging in hate speech or who is engaging in a threat for example. So I think that's one of the sort of big difficulties around it. I think the other thing is that right? Like it is really just the difference between like digital and a lot of context, right? If you are a trust and safety, this is operationally focused. If you're a trust and safety employee, you can scan through a conversation that maybe spans, I don't know, a couple hours, you can do an investigation on someone's account Pretty quickly. If you're, you know, in a clubhouse session and someone's like, hey, someone said a bad thing, I don't know somewhere around the 25-30 minute mark. You're listening to that for at least you know, the five minutes, right? If not more than that, you probably want to listen to a little bit more than that for context. That is sort of a limiting factor. So
	- link:: [https://www.airr.io/quote/60816b5f23577f36663a7e8c](https://www.airr.io/quote/60816b5f23577f36663a7e8c)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: don't know somewhere around the 25-30 minute mark. You're listening to that for at least you know, the five minutes, right? If not more than that, you probably want to listen to a little bit more than that for context. That is sort of a limiting factor. So I, I think fundamentally part of it is that clubhouse really was the Like audio has been around in a lot of these spaces for a long time to, to to your points, right? Like it's not like we were suddenly the guy is like, I have something brand new. It's called talking to each other. But I think historically it has mostly been either 1-1 or it has been in A. B, two B a business to business context. Right? And so in both of those cases, sort of your content moderation issues are lower. I think now with, with
	- link:: [https://www.airr.io/quote/60816b7723577f07433a7e8f](https://www.airr.io/quote/60816b7723577f07433a7e8f)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: of those cases, sort of your content moderation issues are lower. I think now with, with clubhouse, especially now everyone's like, hey, there's a lot of interest in that particular type of engagement where you have a, you know, some people are the host, but anyone can join, it could potentially go to anyone and it's open to anyone. I think we are seeing this sort of like new wave of content moderation issues and I strongly suspect right, like every company will have to deal with these on their own spaces. I mean, I for one am very curious how they will respond to this because you can imagine, right, like Text is scalable to 10,000 rows or 10 million messages in a way that audio is not scalable if you have 10,000 rooms going on at the same time,
	- link:: [https://www.airr.io/quote/60816ba623577f325c3a7e92](https://www.airr.io/quote/60816ba623577f325c3a7e92)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: this or someone should have done something about this, right? Like that text last forever with these audio spaces that are sort of right? Like these, I guess they are conferences effectively right like that are happening sort of ephemeral e hypothetically you're blast radius is actually much lesson, it's only the people in the cause that are that are hearing it. I mean very I actually saw it on twitter this morning, there's been some reports of some more anti Semitic clubhouse rooms and it's really hard because for me, I'm like well I can't verify that, right? And I don't know if clubhouse can now verify that, given that the room presumably is over, it sort of happened and it may have injured the people inside that room, but all anyone else has our our second hand reports of it. So it's
	- link:: [https://www.airr.io/quote/60816bb623577f61aa3a7e95](https://www.airr.io/quote/60816bb623577f61aa3a7e95)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: rooms and it's really hard because for me, I'm like well I can't verify that, right? And I don't know if clubhouse can now verify that, given that the room presumably is over, it sort of happened and it may have injured the people inside that room, but all anyone else has our our second hand reports of it. So it's this it's I think it's the sword that cuts both ways. Right on one hand, the blast radius is smaller. You can't like, you know, retweet this terrible thing and have it go to millions or billions of people on the other hand, however right, it means that the people that are in that room are potentially the only people that are able to report this thing and if you are doing a bad thing, maybe you can evade enforcement just by effectively keeping your room short and doing it over and over. 
	  
	  Speaker 1: Yes. Although I
	- link:: [https://www.airr.io/quote/60816bd141de78f374cd79d0](https://www.airr.io/quote/60816bd141de78f374cd79d0)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: room short and doing it over and over. 
	  
	  Speaker 1: Yes. Although I do think that there is a really important conversation to be had that we've barely started yet about do we really want the same standards for content moderation or trust and safety across every platform and every kind of afford inst like it might be that we have different standards for text that's forever and sitting there and ephemeral content which is you know, as you said, talking to each other online and so it's not clear to me that you know, we necessarily want uniformity across all of the different platform types, but we've barely started that conversation yet. So I expect that to be something that we explore more in coming years. One of the things
	- link:: [https://www.airr.io/quote/60816bea41de7804afcd79d2](https://www.airr.io/quote/60816bea41de7804afcd79d2)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 1: that we explore more in coming years. One of the things that you were talking about just then about being able to verify if something was said and also the scalability of content moderation in audio. One of things that that gets to is because it's not necessarily realistic or desirable to have platform moderators sitting and listening in on every conversation that happens on the platform to make sure that nothing that infringes the guidelines is said. And because automated tools as we were exploring, is so much blunter and still basically inadequate and not up to the task, a lot more of the responsibility falls to the channel and server moderators and the community in general. And
	- link:: [https://www.airr.io/quote/60816d1e23577f308e3a7ec6](https://www.airr.io/quote/60816d1e23577f308e3a7ec6)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: call back to, for example, the the paradox of tolerance here, Right. And and those ideas, but I think that the fundamental sort of nature of it is that if you sort of push this and there was some discussion actually in Inside Channel yesterday about this, if you push this to sort of the middleware and you're like, hey, I'm going to let anything go. And then there's, there's this sort of middle layer, right of software or what have you, where you can choose to customize your experience like Yes, in one sense you're, you're pushing that responsibility onto your users and you're allowing users perhaps with good tooling to customize exactly how their experiences want to be. I think the real question though is like, right shouldn't you as the government be setting a floor to what that acceptable behavior is, right? Like at some level, I think the
	- link:: [https://www.airr.io/quote/60816d2a41de785224cd7a05](https://www.airr.io/quote/60816d2a41de785224cd7a05)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: right of software or what have you, where you can choose to customize your experience like Yes, in one sense you're, you're pushing that responsibility onto your users and you're allowing users perhaps with good tooling to customize exactly how their experiences want to be. I think the real question though is like, right shouldn't you as the government be setting a floor to what that acceptable behavior is, right? Like at some level, I think the real question is is it reasonable for any platform to say ah yes. Like you can say you can make death threats because if the user wants to opt out of it, they can just opt out of it. Like that feels wrong to me personally, because I think that the platform, the government has a responsibility to step in. I don't think that that
	- link:: [https://www.airr.io/quote/60816d4b23577f86583a7ec8](https://www.airr.io/quote/60816d4b23577f86583a7ec8)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: that right, the ability and and maybe the sort of optimal nature for me is a government that protects users in a way where they feel comfortable, they feel safe in expressing themselves. And that is I think just not status quo and a lot of these major platforms right now that if you are a minority, for example, you do not feel safe necessarily expressing yourself because when you do you immediately get all sorts of like vicious harassment. Would the world be better if those people had tools to better sort of save themselves? Like? I think so. Yes. Right. And there are initiatives like block party is one of the apps, I think that is currently in stealth, that sort of aims to do some of that, but I think the fundamental responsibility should actually lie on the platform itself.
	- link:: [https://www.airr.io/quote/60816d8823577f85e43a7eca](https://www.airr.io/quote/60816d8823577f85e43a7eca)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 2: wondering, you know, isn't that basically just the audio equivalent of the twitter mute function? But I wanted to ask you, you know, what do you think of tools like these and what is the capability of automated moderation when it comes to audio? 
	  
	  Speaker 0: There's definitely a lot of research going into it. I'm reasonably familiar with the with the intel offering. I mean, I think that part of it and this is just sort of generally from what I know in most cases what's happening is that they're taking the audio and they're doing a transcribe right into text and then they're running the ai on the text. And so it sort of goes back to that earlier conversation. You lose like some of the tone that exists, right? You lose some of the emphasis that that exists, right? It's it's a very different
	- link:: [https://www.airr.io/quote/60816d9541de78513acd7a0c](https://www.airr.io/quote/60816d9541de78513acd7a0c)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: I think that part of it and this is just sort of generally from what I know in most cases what's happening is that they're taking the audio and they're doing a transcribe right into text and then they're running the ai on the text. And so it sort of goes back to that earlier conversation. You lose like some of the tone that exists, right? You lose some of the emphasis that that exists, right? It's it's a very different statement if I say, I'm gonna kill someone today and you know, I'm going to kill someone today, right? And I think that's something that most to my knowledge, maybe none of the current sort of Ai systems that exist do that. They're all sort of taking that, turning it into texas and saying, okay, is the text violative? So I don't think the ai is quite there yet, but I think it's sort of like even if we're looking at just like
	- link:: [https://www.airr.io/quote/60816db241de7899cacd7a13](https://www.airr.io/quote/60816db241de7899cacd7a13)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: ai right? I think that context is still incredibly hard for for the Ai systems that everyone you know has the ability to sort of purchase or that everyone is sort of implementing right now. I think it's still hard because you can study all of the data in the world and and see these like billions of interactions. But it turns out that like human interaction, right? Is complex and is frequently subtle. And if someone is using a specific word right, like they may be using that word as a racial slur, they may be using that word as part of a rap lyric, they may be using that word as a member of the in group. They may be using that word in sort of a journalistic sense. Right? And I think it's that context that is really hard. And
	- link:: [https://www.airr.io/quote/60816dd023577f6b243a7ed2](https://www.airr.io/quote/60816dd023577f6b243a7ed2)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: you know, facebook's transparency reports. I think you can see this sort of from all of the vendors out there, like the ai that exists is going to be overbroad, it is going to take down things that it should not take down. And I think it is just a sort of a question right of whether, I think most companies are okay with that because it turns out that there's no, you know, there's no there's no liability for taking things down when you didn't need to take them down. And I think in a lot of cases, in the trust and safety space, you would rather have taken something down, right that you needed to take down rather than leave something up. And so it's a little bit disappointing. I think in some cases that that is sort of the default model, but that does feel like the default and current model and I'm not sure that that's going to change anytime soon.
	- link:: [https://www.airr.io/quote/60816ddc23577f1e553a7ed4](https://www.airr.io/quote/60816ddc23577f1e553a7ed4)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: because it turns out that there's no, you know, there's no there's no liability for taking things down when you didn't need to take them down. And I think in a lot of cases, in the trust and safety space, you would rather have taken something down, right that you needed to take down rather than leave something up. And so it's a little bit disappointing. I think in some cases that that is sort of the default model, but that does feel like the default and current model and I'm not sure that that's going to change anytime soon. 
	  
	  Speaker 1: So discord saw a big jump in violent extremism takedowns in the second half of 2020, the team proactively removed 1,504 servers for violent extremism in the second half of 2020, which was a nearly 93 increase from the first half of the year.
	- link:: [https://www.airr.io/quote/60816de223577f25143a7ed7](https://www.airr.io/quote/60816de223577f25143a7ed7)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: have taken something down, right that you needed to take down rather than leave something up. And so it's a little bit disappointing. I think in some cases that that is sort of the default model, but that does feel like the default and current model and I'm not sure that that's going to change anytime soon. 
	  
	  Speaker 1: So discord saw a big jump in violent extremism takedowns in the second half of 2020, the team proactively removed 1,504 servers for violent extremism in the second half of 2020, which was a nearly 93 increase from the first half of the year. The transparency report noted that this increase can be attributed to the expansion of our anti extremism efforts as well as growing trends in the online extremism space. And I'm wondering
	- link:: [https://www.airr.io/quote/6081702e41de7833afcd7a3e](https://www.airr.io/quote/6081702e41de7833afcd7a3e)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: and it is always hard I think in situations like this because when you say, oh yes, like we used to you know remove this amount and now we remove this amount. There's always this question of like, okay, does that mean that the activity itself drastically increased or did you just get better at catching it and you weren't catching it before? And you know, I don't work there anymore and I don't have a very good guess as to what that is, but I I think, you know, in almost all cases it is going to be a little bit of Coleman and a little bit of Colombia, right, you put more resources into finding it, maybe you're able to sort of like investigate more deeply in certain cases, right? At the same time as the world is reacting and maybe there is more of it than there was to find. 
	  
	  Speaker 2: So given that you're both a
	- link:: [https://www.airr.io/quote/6081708723577f0d613a7eeb](https://www.airr.io/quote/6081708723577f0d613a7eeb)
	  on:: [[2021/04/23]]
	  tags:: 
	  Speaker 0: the big players have much more of it to deal with, but again, they can throw a billion dollars at it and then effectively consider it dealt with, Right? I think that for these small, for the small startups out there for the for the non incumbents, right? For the companies that are, you know, maybe in this sort of medium space where you have some users, but you're still trying to, you know, figure things out, you're still trying to figure out like what your company will look like over the next couple of years. I think it is a much, it is just much harder. And I think that certainly I've seen hopefully not very successful bills that would effectively, you know, legislate Companies like to sort out of existence. So I do think that that is a very real danger. I think there are, you know, there's a lot of ink that's been shed